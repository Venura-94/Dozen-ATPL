from src.connectors.llm import LLM
from src.models.result_with_sources_used import ResultWithSourcesUsed
from src.models.chunk import Chunk
from src.models.mcq import MCQ
from src.connectors.embeddings import Embeddings
from src.connectors.vectorstore import ChromaLocal
from src import config


def __generate_keywords_to_fetch_documents(mcq_question: str, correct_answer: str, answer_seeking_explanation: str = None) -> str:
    """We give the MCQ question and correct answer to the LLM, and tell it to generate keywords in order to retrieve context documents from a 
    vectorstore, so that it can explaint why `answer_seeking_explanation` is right or wrong. Note that this LLM call does not do the explaning,
    it just generates the keywords so that the most appropriate chunks can be retrieved from the vectorstore using a similarity search.

    Args:
        mcq_question (str): The MCQ eustion
        correct_answer (str): The correct answer to the MCQ
        answer_seeking_explanation (str, optional): The answer from the list of answers that needs explanation. 
            Obviously, this can be the correct answer itself. If ommited, we will assume that you are trying to get the
             explanation for the `correct_answer` itself. Defaults to None.

    Returns:
        str: keywords generated by the LLM as a single string
    """

    prompt_string = f"""
    Consider the MCQ Question denoted between triple backticks.
    ```{mcq_question}```
    The correct answer is '{correct_answer}'.
    """

    if (not answer_seeking_explanation) or (answer_seeking_explanation == correct_answer):
        task = f"""
        Your task is to generate text containing keywords to search for appropriate documents from a vectorstore, 
        so that someone else can explain why '{correct_answer}' is the correct answer to this MCQ.
        """
    else:
        task = f"""
        Your task is to generate text containing keywords to search for appropriate documents from a vectorstore, 
        so that someone else can explain why '{answer_seeking_explanation}' is not the correct answer to this MCQ.
        """

    prompt_string += task
    prompt_string += """
    Reply only with the keywords as a string. Don't tell me anything else.
    """

    llm = LLM.get_openai_client()
    response = llm.responses.create(
        input=prompt_string,
        model=config.LLM_MODEL
    )
    keywords = response.output_text
    print(f'KEYWORDS: {keywords}')
    return keywords


def __get_explanation_with_sources(mcq_question: str, correct_answer: str, answer_seeking_explanation: str = None) -> tuple[str,list[Chunk]]:
    """Performs RAG and gets the explanation of why an MCQ option is incorrect or correct, with sources.

    Args:
        mcq_question (str): The MCQ question.
        correct_answer (str): The correct answer to the question.
        answer_seeking_explanation (str, optional): The MCQ option/answer to which you want an explanation generated. 
         If ommited, we assume that you seek the explanation to the `correct_answer` itself. Defaults to None.

    Returns:
        tuple[str,list[Chunk]]: explanation, sources
    """

    keywords = __generate_keywords_to_fetch_documents(mcq_question, correct_answer, answer_seeking_explanation)

    query_embedding = Embeddings.embed_texts([keywords])[0]
    context = ChromaLocal.query_collection("book8", query_embedding)
    
    prompt_string = f"""
    Consider the MCQ Question denoted between triple backticks.
    ```{mcq_question}```
    The correct answer is '{correct_answer}'.

    Use ONLY the following pieces of retrieved context to explain why 
    """

    if (not answer_seeking_explanation) or (answer_seeking_explanation == correct_answer):
        task = f"""
        {correct_answer} is the correct answer to this MCQ.
        """
    else:
        task = f"""
        {answer_seeking_explanation} is not the correct answer to this MCQ.
        """

    prompt_string += task
    prompt_string += """If you don't know the answer, say that you don't know.
    You don't need to mention the fact that you used the context in your answer. Keep your answer as short as possible.
    The retrieved pieces of context are numbered.
    """
    prompt_string += 'CONTEXT:  \n'
    for i, chunk in enumerate(context):
        prompt_string += f"{i + 1}. {chunk.markdown}\n"

    sources: list[dict] = []
    llm_client = LLM.get_openai_client()
    response = llm_client.responses.parse(
        model=config.LLM_MODEL,
        input=prompt_string,
        text_format=ResultWithSourcesUsed,
    )
    result = response.output_parsed
    print(result)

    # only get the sources that were useful for generating the answer
    for n in result.context_pieces_used:
        index = n-1
        chunk = context[index]
        sources.append(chunk)

    return (result.answer, sources)

def generate_llm_explanations_and_sources(mcq: MCQ):
    """Generates explanations with sources for each possible answer of the MCQ. 
    
    The explanations explain why the possible answer is right or wrong.

    Args:
        mcq (MCQ): _description_
    """
    mcq.explanations = []; mcq.sources = []

    for possible_answer in mcq.possible_answers:
        explanation, sources = __get_explanation_with_sources(
            mcq_question=mcq.question,
            correct_answer=mcq.possible_answers[mcq.correct_answer_index],
            answer_seeking_explanation=possible_answer
        )
        mcq.explanations.append(explanation)
        mcq.sources.append(sources)